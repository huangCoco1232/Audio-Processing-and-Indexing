"""
intergration.pyæ˜¯åœ¨web.pyçš„åŸºç¡€ä¸Šä¿®æ”¹çš„ã€‚
æ¶æ„è®¾è®¡ï¼ˆæ‰‹ä¸Šæ²¡æœ‰æ¨¡å‹æºè¿˜éœ€è¦åœ¨æœ‰æ¨¡å‹æºçš„å‰æä¸‹è°ƒè¯•ã€‚å¦‚æœå¯ä»¥ç”¨å°±ä»£æ›¿web.py
ä»£ç ä½¿ç”¨äº†ç›¸å¯¹è·¯å¾„ current_dir æ¥å®šä½ CosyVoice æ–‡ä»¶å¤¹ã€‚ å› æ­¤intergration.pyéœ€è¦æ”¾åœ¨å’Œ CosyVoice æ–‡ä»¶å¤¹ åŒçº§ çš„ç›®å½•ä¸‹ã€‚
root/
â”œâ”€â”€ CosyVoice/
â”œâ”€â”€ llm_fastapi.py
â”œâ”€â”€ model_loader.py
â””â”€â”€ intergration.py
--------------------------------------------------------------------------------
1. å¯åŠ¨ LLM æœåŠ¡ï¼š
   - åœ¨åå°è¿è¡Œ llm_fastapi.pyã€‚
   - ç›®çš„ï¼šä½œä¸ºç‹¬ç«‹æœåŠ¡è¿è¡Œï¼Œé¿å…é˜»å¡ Streamlit ä¸»è¿›ç¨‹ï¼Œé˜²æ­¢ç•Œé¢å¡é¡¿ã€‚

2. å‰ç«¯æ”¹é€  (web.py)ï¼š
   A. åˆå§‹åŒ– TTSï¼š
      - åœ¨ web.py å¯åŠ¨æ—¶åŠ è½½ CosyVoice æ¨¡å‹ã€‚
      - å…³é”®ç‚¹ï¼šåˆ©ç”¨ @st.cache_resource è£…é¥°å™¨ï¼Œç¡®ä¿æ¨¡å‹åªåŠ è½½ä¸€æ¬¡ï¼Œé¿å…é‡å¤åŠ è½½æ¶ˆè€—èµ„æºã€‚

   B. è¯·æ±‚ LLMï¼š
      - ä½¿ç”¨ Python requests åº“å‘ llm_fastapi å‘é€ HTTP POST è¯·æ±‚ã€‚
      - æµç¨‹ï¼šå‰ç«¯è¾“å…¥ -> å‘é€è¯·æ±‚ -> è·å–æ–‡æœ¬å›å¤ã€‚

   C. ç”Ÿæˆè¯­éŸ³ï¼š
      - å°† LLM è¿”å›çš„æ–‡æœ¬å†…å®¹ä¼ é€’ç»™ CosyVoice æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚

   D. æ’­æ”¾ï¼š
      - è·å–æ¨ç†åçš„éŸ³é¢‘æ•°æ®ï¼Œä½¿ç”¨ st.audio ç»„ä»¶åœ¨å‰ç«¯æ’­æ”¾ã€‚
--------------------------------------------------------------------------------
"""

import os
import time
import json
import requests
import torch
import streamlit as st
import whisper
from audiorecorder import audiorecorder
from io import BytesIO
import sys
import torchaudio

# =================é…ç½®åŒºåŸŸ=================
# è®¾ç½® CosyVoice è·¯å¾„ (å‚è€ƒ new.py)
current_dir = os.path.dirname(os.path.abspath(__file__))
cosyvoice_root = os.path.join(current_dir, "CosyVoice")
sys.path.insert(0, cosyvoice_root)
sys.path.insert(0, os.path.join(cosyvoice_root, "third_party", "Matcha-TTS"))
sys.path.insert(0, os.path.join(cosyvoice_root, "third_party", "AcademiCodec"))

from cosyvoice.cli.cosyvoice import CosyVoice2
from cosyvoice.utils.file_utils import load_wav

# LLM API åœ°å€ (å‡è®¾ llm_fastapi.py è¿è¡Œåœ¨æœ¬åœ° 8000 ç«¯å£)
LLM_API_URL = "http://localhost:8000/chat"

# å½•éŸ³ä¿å­˜ä¸ TTS è¾“å‡ºç›®å½•
OUTPUT_DIR = "output"
os.makedirs(OUTPUT_DIR, exist_ok=True)
# ç« é±¼å“¥å‚è€ƒéŸ³é¢‘è·¯å¾„ (å‚è€ƒ audio_convert.py)
REF_AUDIO_PATH = os.path.join(cosyvoice_root, "example_audio", "squidward_16k_clean.wav")
# ç« é±¼å“¥å‚è€ƒæ–‡æœ¬ (å‚è€ƒ new.pyï¼Œç”¨äº Zero-shot æç¤º)
REF_TEXT_PROMPT = (
    "look spongebob i told you use your net and go fish! "
    "happy birthday, SpongeBob SquarePants! are you insane. "
    "Hahahaha,Hahahaha,Hahahaha! I'll ever get surrounded by such loser neighbors! "
    "Hahahaha!Hahahaha!Hahahaha! spongebob, can we lower the volume please?"
)

st.set_page_config(page_title="Squidward Voice Bot", layout="wide")
st.title("ğŸ™ Squidward Voice Chat (Phi-4 + CosyVoice2)")


# =================æ¨¡å‹åŠ è½½=================

@st.cache_resource
def init_asr():
    """åŠ è½½ Whisper ASR æ¨¡å‹"""
    print("Loading Whisper...")
    return whisper.load_model("small")


@st.cache_resource
def init_tts():
    """åŠ è½½ CosyVoice2 TTS æ¨¡å‹ (åªåŠ è½½ä¸€æ¬¡)"""
    print("Loading CosyVoice2...")
    model_path = os.path.join(cosyvoice_root, "pretrained_models", "CosyVoice2-0.5B")
    # æ³¨æ„ï¼šæ ¹æ®ä½ çš„æ˜¾å­˜æƒ…å†µï¼Œfp16 å¯ä»¥è®¾ä¸º True
    cosyvoice = CosyVoice2(model_path, load_jit=False, load_trt=False, fp16=False)

    # é¢„åŠ è½½å‚è€ƒéŸ³é¢‘
    if os.path.exists(REF_AUDIO_PATH):
        prompt_speech_16k = load_wav(REF_AUDIO_PATH, 16000)
    else:
        st.error(f"æœªæ‰¾åˆ°å‚è€ƒéŸ³é¢‘: {REF_AUDIO_PATH}")
        prompt_speech_16k = None

    return cosyvoice, prompt_speech_16k


# =================åŠŸèƒ½å‡½æ•°=================

def get_llm_response(user_text):
    """è°ƒç”¨ llm_fastapi æ¥å£"""
    try:
        payload = {"messages": [user_text]}
        response = requests.post(LLM_API_URL, json=payload)
        if response.status_code == 200:
            return response.json().get("reply", "")
        else:
            return f"Error: LLM API returned {response.status_code}"
    except Exception as e:
        return f"Error connecting to LLM: {str(e)}"


def generate_audio(tts_model, prompt_speech, text_to_say, output_filename):
    """ä½¿ç”¨ CosyVoice ç”ŸæˆéŸ³é¢‘"""
    if not tts_model or not prompt_speech:
        return None

    all_chunks = []
    # ä½¿ç”¨ zero_shot æ¨ç†
    for out in tts_model.inference_zero_shot(
            text_to_say,
            REF_TEXT_PROMPT,
            prompt_speech,
            stream=False
    ):
        all_chunks.append(out["tts_speech"])

    if all_chunks:
        full_audio = torch.cat(all_chunks, dim=-1)
        save_path = os.path.join(OUTPUT_DIR, output_filename)
        torchaudio.save(save_path, full_audio, tts_model.sample_rate)
        return save_path
    return None


# =================ä¸»ç¨‹åº=================

def main():
    # 1. åˆå§‹åŒ–æ¨¡å‹
    asr_model = init_asr()
    tts_model, prompt_speech = init_tts()

    # 2. åˆå§‹åŒ–èŠå¤©è®°å½•
    if "messages" not in st.session_state:
        st.session_state.messages = []
        # åˆå§‹é—®å€™
        st.session_state.messages.append({
            "role": "assistant",
            "content": "Oh, great. Another neighbor. What do you want? (I'm listening...)"
        })

    # 3. æ˜¾ç¤ºå†å²æ¶ˆæ¯
    for message in st.session_state.messages:
        avatar = 'ğŸ§‘â€ğŸ’»' if message["role"] == "user" else 'ğŸ™'
        with st.chat_message(message["role"], avatar=avatar):
            st.markdown(message["content"])
            # å¦‚æœå†å²æ¶ˆæ¯é‡Œæœ‰éŸ³é¢‘è·¯å¾„ï¼Œä¹Ÿæ˜¾ç¤ºå‡ºæ¥ï¼ˆå¯é€‰ï¼‰
            if "audio" in message:
                st.audio(message["audio"])

    # 4. å½•éŸ³éƒ¨åˆ†
    st.markdown("---")
    audio = audiorecorder("Click to Record", "Recording...")

    if len(audio) > 0:
        # ç®€å•çš„å»é‡é€»è¾‘
        buf = BytesIO()
        audio.export(buf, format="wav")
        audio_bytes = buf.getvalue()

        current_len = len(audio_bytes)
        if st.session_state.get("last_audio_len") != current_len:
            st.session_state["last_audio_len"] = current_len

            # --- Step A: ä¿å­˜ç”¨æˆ·å½•éŸ³ ---
            timestamp = time.strftime("%H%M%S")
            user_wav_path = os.path.join(OUTPUT_DIR, f"user_{timestamp}.wav")
            with open(user_wav_path, "wb") as f:
                f.write(audio_bytes)

            # --- Step B: ASR (Whisper) ---
            with st.spinner("Listening (Whisper)..."):
                result = asr_model.transcribe(user_wav_path, language="zh")  # æˆ– auto
                user_text = result.get("text", "").strip()

            if user_text:
                # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
                st.session_state.messages.append({"role": "user", "content": user_text})
                with st.chat_message("user", avatar='ğŸ§‘â€ğŸ’»'):
                    st.markdown(user_text)

                # --- Step C: LLM (Phi-4 via API) ---
                with st.spinner("Thinking (Phi-4)..."):
                    reply_text = get_llm_response(user_text)

                # --- Step D: TTS (CosyVoice2) ---
                tts_audio_path = None
                with st.spinner("Speaking (Squidward TTS)..."):
                    tts_filename = f"reply_{timestamp}.wav"
                    tts_audio_path = generate_audio(tts_model, prompt_speech, reply_text, tts_filename)

                # æ˜¾ç¤ºåŠ©æ‰‹æ¶ˆæ¯
                msg_data = {"role": "assistant", "content": reply_text}
                if tts_audio_path:
                    msg_data["audio"] = tts_audio_path

                st.session_state.messages.append(msg_data)

                with st.chat_message("assistant", avatar='ğŸ™'):
                    st.markdown(reply_text)
                    if tts_audio_path:
                        st.audio(tts_audio_path)
            else:
                st.warning("Sorry, please say it again.")

    # æ¸…ç©ºæŒ‰é’®
    if st.button("Clear Conversation"):
        st.session_state.messages = []
        st.rerun()


if __name__ == "__main__":
    main()